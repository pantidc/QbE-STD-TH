{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mAP metric, you will need ground truth binary labels and predicted scores for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Calculate mean Average Precision\n",
    "    y_true: list of arrays, Ground truth labels for each query\n",
    "    y_scores: list of arrays, Predicted scores for each query\n",
    "    \"\"\"\n",
    "    aps = [average_precision_score(y_t, y_s) for y_t, y_s in zip(y_true, y_scores)]\n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the list corresponds to the labels or scores for one query\n",
    "y_true =  #list of numpy arr\n",
    "y_scores =  #list of numpy arr\n",
    "\n",
    "mAP = calculate_map(y_true, y_scores)\n",
    "print(f\"Mean Average Precision: {mAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision at 5 (P@5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The P@5 metric considers only the top 5 retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores, k=5):\n",
    "    \"\"\"\n",
    "    Compute Precision at k for each query\n",
    "    y_true: list of arrays, Ground truth labels for each query\n",
    "    y_scores: list of arrays, Predicted scores for each query\n",
    "    \"\"\"\n",
    "    p_at_k = []\n",
    "    for y_t, y_s in zip(y_true, y_scores):\n",
    "        top_indices = np.argsort(y_s)[::-1][:k]\n",
    "        top_k_true = y_t[top_indices]\n",
    "        p_at_k.append(np.sum(top_k_true) / k)\n",
    "    return np.mean(p_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same y_true and y_scores arrays as placeholders for demonstration\n",
    "P_at_5 = precision_at_k(y_true, y_scores, k=5)\n",
    "print(f\"Precision at 5: {P_at_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output the mAP and P@5 scores, providing a holistic view of your word-searching system's performance in terms of both overall retrieval accuracy and precision at the top 5 retrieved instances. These metrics should offer valuable insights into the retrieval performance and precision of your word-searching system, enabling further refinements or confirming its effectiveness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
